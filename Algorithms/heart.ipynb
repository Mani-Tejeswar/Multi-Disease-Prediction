{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509b5a4c-3f8c-4b37-8b76-28e607fac78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "First 5 rows of the dataset:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n",
      "\n",
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n",
      "\n",
      "Applying preprocessing steps to training data...\n",
      "Outliers capped for all features.\n",
      "\n",
      "First 5 rows of scaled features (after outlier treatment):\n",
      "        age       sex        cp  trestbps      chol  fbs   restecg   thalach  \\\n",
      "0  0.952197  0.681005  1.973123  0.828728 -0.255601  0.0 -1.005832  0.013543   \n",
      "1 -1.915313  0.681005  1.002577 -0.077351  0.102487  0.0  0.898962  1.641748   \n",
      "2 -1.474158 -1.468418  0.032031 -0.077351 -0.866457  0.0 -1.005832  0.981665   \n",
      "3  0.180175  0.681005  0.032031 -0.681403 -0.192409  0.0  0.898962  1.245698   \n",
      "4  0.290464 -1.468418 -0.938515 -0.681403  2.293143  0.0  0.898962  0.585615   \n",
      "\n",
      "      exang   oldpeak     slope        ca      thal  \n",
      "0 -0.696631  1.150938 -2.274579 -0.760077 -2.194678  \n",
      "1 -0.696631  2.233684 -2.274579 -0.760077 -0.528043  \n",
      "2 -0.696631  0.338879  0.976352 -0.760077 -0.528043  \n",
      "3 -0.696631 -0.202494  0.976352 -0.760077 -0.528043  \n",
      "4  1.435481 -0.382951  0.976352 -0.760077 -0.528043  \n",
      "\n",
      "Original Training set size: 242 samples\n",
      "Original Testing set size: 61 samples\n",
      "Original Outcome distribution in training set:\n",
      "target\n",
      "1    0.545455\n",
      "0    0.454545\n",
      "Name: proportion, dtype: float64\n",
      "Original Outcome distribution in testing set:\n",
      "target\n",
      "1    0.540984\n",
      "0    0.459016\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Applying SMOTE to balance the training data...\n",
      "\n",
      "Resampled Training set size: 264 samples\n",
      "Resampled Outcome distribution in training set:\n",
      "target\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Starting Enhanced Hyperparameter Tuning for SVM on resampled data (this might take a moment)...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Best parameters found: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best cross-validation F1-weighted score on resampled data: 0.8633\n",
      "\n",
      "Best SVM model trained successfully using best parameters on resampled data.\n",
      "\n",
      "Model Accuracy on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment): 0.8361\n",
      "\n",
      "Classification Report on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.79        28\n",
      "           1       0.78      0.97      0.86        33\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.87      0.82      0.83        61\n",
      "weighted avg       0.86      0.84      0.83        61\n",
      "\n",
      "\n",
      "Confusion Matrix on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment):\n",
      "[[19  9]\n",
      " [ 1 32]]\n",
      "\n",
      "Model saved to heart_disease_svm_model.joblib\n",
      "Scaler saved to heart_disease_scaler.joblib\n",
      "Training IQR bounds for outlier treatment saved to heart_disease_train_iqr_bounds.joblib\n",
      "\n",
      "--- Example: Loading saved model and predicting on new data ---\n",
      "Model, scaler, and IQR bounds loaded successfully.\n",
      "\n",
      "New patient data for prediction:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   58    1   2       140   240    0        1      150      0      1.5      2   \n",
      "\n",
      "   ca  thal  \n",
      "0   1     2  \n",
      "\n",
      "Predicted Outcome for new patient: Heart Disease\n",
      "Prediction Probability (No Heart Disease, Heart Disease): [0.2829245 0.7170755]\n",
      "\n",
      "New patient data 2 for prediction:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   35    0   0       120   200    0        0      170      0      0.0      2   \n",
      "\n",
      "   ca  thal  \n",
      "0   0     2  \n",
      "\n",
      "Predicted Outcome for new patient 2: Heart Disease\n",
      "Prediction Probability (No Heart Disease, Heart Disease): [0.08896816 0.91103184]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE # Import SMOTE\n",
    "import joblib # For saving and loading models\n",
    "\n",
    "# --- 1. Load the dataset ---\n",
    "# The dataset 'heart.csv' is assumed to be available in the environment.\n",
    "try:\n",
    "    df = pd.read_csv('heart.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset information:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'heart.csv' not found. Please ensure the file is uploaded correctly.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Data Preprocessing ---\n",
    "\n",
    "# Identify features (X) and target (y)\n",
    "# For heart.csv, 'target' is typically the target variable (0 for no disease, 1 for disease)\n",
    "# All other columns are features.\n",
    "X = df.drop('target', axis=1) # Assuming 'target' is the outcome column for heart disease\n",
    "y = df['target']\n",
    "\n",
    "# Note: Unlike the diabetes dataset, the heart disease dataset typically does not\n",
    "# have 0s representing missing values in critical columns like Glucose, BloodPressure, etc.\n",
    "# Therefore, the specific 'cols_to_replace_zero' imputation step is removed.\n",
    "# If your 'heart.csv' has missing values, you would need to add appropriate handling here\n",
    "# (e.g., df.fillna(df.mean()), or more advanced imputation).\n",
    "\n",
    "# Function to apply preprocessing steps (outlier treatment, scaling)\n",
    "# This function will be used for both training and new data prediction\n",
    "def preprocess_data(data_df, scaler=None, is_training=True, train_iqr_bounds=None):\n",
    "    processed_df = data_df.copy()\n",
    "\n",
    "    # Outlier Treatment (Capping using IQR method)\n",
    "    # For new data, use IQR bounds calculated from training data\n",
    "    current_iqr_bounds = {}\n",
    "    if is_training:\n",
    "        for column in processed_df.columns:\n",
    "            Q1 = processed_df[column].quantile(0.25)\n",
    "            Q3 = processed_df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            processed_df[column] = processed_df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "            current_iqr_bounds[column] = {'lower': lower_bound, 'upper': upper_bound}\n",
    "    else:\n",
    "        if train_iqr_bounds is None:\n",
    "            raise ValueError(\"train_iqr_bounds must be provided for new data preprocessing.\")\n",
    "        for column in processed_df.columns:\n",
    "            lower_bound = train_iqr_bounds[column]['lower']\n",
    "            upper_bound = train_iqr_bounds[column]['upper']\n",
    "            processed_df[column] = processed_df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "        current_iqr_bounds = train_iqr_bounds # No need to recalculate for new data\n",
    "\n",
    "    # Feature Scaling\n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(processed_df)\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"Scaler must be provided for new data preprocessing.\")\n",
    "        scaled_data = scaler.transform(processed_df)\n",
    "\n",
    "    processed_df_scaled = pd.DataFrame(scaled_data, columns=processed_df.columns)\n",
    "\n",
    "    if is_training:\n",
    "        return processed_df_scaled, scaler, current_iqr_bounds\n",
    "    else:\n",
    "        return processed_df_scaled\n",
    "\n",
    "\n",
    "# Preprocess training data\n",
    "print(\"\\nApplying preprocessing steps to training data...\")\n",
    "X_processed, scaler, train_iqr_bounds = preprocess_data(X, is_training=True)\n",
    "\n",
    "print(\"Outliers capped for all features.\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of scaled features (after outlier treatment):\")\n",
    "print(X_processed.head())\n",
    "\n",
    "# --- 3. Split the data into training and testing sets ---\n",
    "# We'll use 80% for training and 20% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nOriginal Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Original Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Original Outcome distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Original Outcome distribution in testing set:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "# --- 4. Apply SMOTE to the training data ---\n",
    "print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nResampled Training set size: {X_train_resampled.shape[0]} samples\")\n",
    "print(f\"Resampled Outcome distribution in training set:\\n{y_train_resampled.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# --- 5. Train an SVM Classifier with Enhanced Hyperparameter Tuning on Resampled Data ---\n",
    "print(\"\\nStarting Enhanced Hyperparameter Tuning for SVM on resampled data (this might take a moment)...\")\n",
    "\n",
    "# Define an expanded parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],          # Regularization parameter: expanded range\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],      # Kernel coefficient for 'rbf'\n",
    "    'kernel': ['rbf']                            # Focusing on RBF kernel\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "# Set probability=True to enable predict_proba\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42, class_weight='balanced', probability=True),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the RESAMPLED training data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1-weighted score on resampled data: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best estimator (the SVM model with the best parameters)\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "print(\"\\nBest SVM model trained successfully using best parameters on resampled data.\")\n",
    "\n",
    "# --- 6. Evaluate the best model's performance on the ORIGINAL Test Set ---\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment): {accuracy:.4f}\")\n",
    "\n",
    "# Generate a classification report (precision, recall, f1-score)\n",
    "print(\"\\nClassification Report on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix on ORIGINAL Test Set (with best parameters, SMOTE, and Outlier Treatment):\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Interpretation of Confusion Matrix:\n",
    "# [[True Negatives (TN)  False Positives (FP)]\n",
    "#  [False Negatives (FN) True Positives (TP)]]\n",
    "# For heart disease prediction:\n",
    "# TN: Correctly predicted no heart disease\n",
    "# FP: Incorrectly predicted heart disease (Type I error)\n",
    "# FN: Incorrectly predicted no heart disease (Type II error, more critical in medical diagnosis)\n",
    "# TP: Correctly predicted heart disease\n",
    "\n",
    "# --- 7. Save the trained model and preprocessing objects ---\n",
    "model_filename = 'heart_disease_svm_model.joblib'\n",
    "scaler_filename = 'heart_disease_scaler.joblib'\n",
    "train_iqr_bounds_filename = 'heart_disease_train_iqr_bounds.joblib'\n",
    "\n",
    "joblib.dump(best_svm_model, model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "joblib.dump(train_iqr_bounds, train_iqr_bounds_filename)\n",
    "\n",
    "print(f\"\\nModel saved to {model_filename}\")\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "print(f\"Training IQR bounds for outlier treatment saved to {train_iqr_bounds_filename}\")\n",
    "\n",
    "\n",
    "# --- 8. Example: Load model and predict on new data ---\n",
    "print(\"\\n--- Example: Loading saved model and predicting on new data ---\")\n",
    "\n",
    "# Load the saved model, scaler, and IQR bounds\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_scaler = joblib.load(scaler_filename)\n",
    "loaded_train_iqr_bounds = joblib.load(train_iqr_bounds_filename)\n",
    "\n",
    "print(\"Model, scaler, and IQR bounds loaded successfully.\")\n",
    "\n",
    "# Create some hypothetical new data for prediction\n",
    "# Ensure the new data has the same columns as the training data, in the same order\n",
    "# Common columns in heart.csv: age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal\n",
    "new_patient_data = pd.DataFrame({\n",
    "    'age': [58],\n",
    "    'sex': [1], # 1 = male, 0 = female\n",
    "    'cp': [2],  # chest pain type (0-3)\n",
    "    'trestbps': [140], # resting blood pressure\n",
    "    'chol': [240], # serum cholestoral in mg/dl\n",
    "    'fbs': [0], # fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "    'restecg': [1], # resting electrocardiographic results (0-2)\n",
    "    'thalach': [150], # maximum heart rate achieved\n",
    "    'exang': [0], # exercise induced angina (1 = yes; 0 = no)\n",
    "    'oldpeak': [1.5], # ST depression induced by exercise relative to rest\n",
    "    'slope': [2], # the slope of the peak exercise ST segment\n",
    "    'ca': [1], # number of major vessels (0-3) colored by flourosopy\n",
    "    'thal': [2] # 1 = normal; 2 = fixed defect; 3 = reversable defect\n",
    "}, index=[0]) # Ensure index is set for single row DataFrame\n",
    "\n",
    "print(\"\\nNew patient data for prediction:\")\n",
    "print(new_patient_data)\n",
    "\n",
    "# Preprocess the new data using the loaded scaler and IQR bounds\n",
    "# Note: is_training=False, and provide the loaded scaler and IQR bounds\n",
    "new_patient_processed = preprocess_data(\n",
    "    new_patient_data,\n",
    "    scaler=loaded_scaler,\n",
    "    is_training=False,\n",
    "    train_iqr_bounds=loaded_train_iqr_bounds\n",
    ")\n",
    "\n",
    "# Make prediction\n",
    "new_prediction = loaded_model.predict(new_patient_processed)\n",
    "prediction_proba = loaded_model.predict_proba(new_patient_processed) # Get probabilities\n",
    "\n",
    "print(f\"\\nPredicted Outcome for new patient: {'Heart Disease' if new_prediction[0] == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Prediction Probability (No Heart Disease, Heart Disease): {prediction_proba[0]}\")\n",
    "\n",
    "# Another example: a patient likely without heart disease\n",
    "new_patient_data_2 = pd.DataFrame({\n",
    "    'age': [35],\n",
    "    'sex': [0],\n",
    "    'cp': [0],\n",
    "    'trestbps': [120],\n",
    "    'chol': [200],\n",
    "    'fbs': [0],\n",
    "    'restecg': [0],\n",
    "    'thalach': [170],\n",
    "    'exang': [0],\n",
    "    'oldpeak': [0.0],\n",
    "    'slope': [2],\n",
    "    'ca': [0],\n",
    "    'thal': [2]\n",
    "}, index=[0])\n",
    "\n",
    "print(\"\\nNew patient data 2 for prediction:\")\n",
    "print(new_patient_data_2)\n",
    "\n",
    "new_patient_processed_2 = preprocess_data(\n",
    "    new_patient_data_2,\n",
    "    scaler=loaded_scaler,\n",
    "    is_training=False,\n",
    "    train_iqr_bounds=loaded_train_iqr_bounds\n",
    ")\n",
    "\n",
    "new_prediction_2 = loaded_model.predict(new_patient_processed_2)\n",
    "prediction_proba_2 = loaded_model.predict_proba(new_patient_processed_2)\n",
    "\n",
    "print(f\"\\nPredicted Outcome for new patient 2: {'Heart Disease' if new_prediction_2[0] == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Prediction Probability (No Heart Disease, Heart Disease): {prediction_proba_2[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d8891-2ea6-4818-b088-2b2f08dc93cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
